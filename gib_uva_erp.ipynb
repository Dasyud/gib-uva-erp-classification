{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gib_uva_erp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mY6brBu6wWoZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Activation, Input, Flatten\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def EEGInception(input_time=1000, fs=128, ncha=8, filters_per_branch=8,\n",
        "                 scales_time=(500, 250, 125), dropout_rate=0.25,\n",
        "                 activation='elu', n_classes=2, learning_rate=0.001):\n",
        "    \"\"\"Keras implementation of EEG-Inception. All hyperparameters and\n",
        "    architectural choices are explained in the original article:\n",
        "    https://doi.org/10.1109/TNSRE.2020.3048106\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_time : int\n",
        "        EEG epoch time in milliseconds\n",
        "    fs : int\n",
        "        Sample rate of the EEG\n",
        "    ncha :\n",
        "        Number of input channels\n",
        "    filters_per_branch : int\n",
        "        Number of filters in each Inception branch\n",
        "    scales_time : list\n",
        "        Temporal scale (ms) of the convolutions on each Inception module.\n",
        "        This parameter determines the kernel sizes of the filters\n",
        "    dropout_rate : float\n",
        "        Dropout rate\n",
        "    activation : str\n",
        "        Activation\n",
        "    n_classes : int\n",
        "        Number of output classes\n",
        "    learning_rate : float\n",
        "        Learning rate\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.models.Model\n",
        "        Keras model already compiled and ready to work\n",
        "    \"\"\"\n",
        "\n",
        "    # ============================= CALCULATIONS ============================= #\n",
        "    input_samples = int(input_time * fs / 1000)\n",
        "    scales_samples = [int(s * fs / 1000) for s in scales_time]\n",
        "\n",
        "    # ================================ INPUT ================================= #\n",
        "    input_layer = Input((input_samples, ncha, 1))\n",
        "\n",
        "    # ========================== BLOCK 1: INCEPTION ========================== #\n",
        "    b1_units = list()\n",
        "    for i in range(len(scales_samples)):\n",
        "        unit = Conv2D(filters=filters_per_branch,\n",
        "                      kernel_size=(scales_samples[i], 1),\n",
        "                      kernel_initializer='he_normal',\n",
        "                      padding='same')(input_layer)\n",
        "        unit = BatchNormalization()(unit)\n",
        "        unit = Activation(activation)(unit)\n",
        "        unit = Dropout(dropout_rate)(unit)\n",
        "\n",
        "        unit = DepthwiseConv2D((1, ncha),\n",
        "                               use_bias=False,\n",
        "                               depth_multiplier=2,\n",
        "                               depthwise_constraint=max_norm(1.))(unit)\n",
        "        unit = BatchNormalization()(unit)\n",
        "        unit = Activation(activation)(unit)\n",
        "        unit = Dropout(dropout_rate)(unit)\n",
        "\n",
        "        b1_units.append(unit)\n",
        "\n",
        "    # Concatenation\n",
        "    b1_out = keras.layers.concatenate(b1_units, axis=3)\n",
        "    b1_out = AveragePooling2D((4, 1))(b1_out)\n",
        "\n",
        "    # ========================== BLOCK 2: INCEPTION ========================== #\n",
        "    b2_units = list()\n",
        "    for i in range(len(scales_samples)):\n",
        "        unit = Conv2D(filters=filters_per_branch,\n",
        "                      kernel_size=(int(scales_samples[i]/4), 1),\n",
        "                      kernel_initializer='he_normal',\n",
        "                      use_bias=False,\n",
        "                      padding='same')(b1_out)\n",
        "        unit = BatchNormalization()(unit)\n",
        "        unit = Activation(activation)(unit)\n",
        "        unit = Dropout(dropout_rate)(unit)\n",
        "\n",
        "        b2_units.append(unit)\n",
        "\n",
        "    # Concatenate + Average pooling\n",
        "    b2_out = keras.layers.concatenate(b2_units, axis=3)\n",
        "    b2_out = AveragePooling2D((2, 1))(b2_out)\n",
        "\n",
        "    # ============================ BLOCK 3: OUTPUT =========================== #\n",
        "    b3_u1 = Conv2D(filters=int(filters_per_branch*len(scales_samples)/2),\n",
        "                   kernel_size=(8, 1),\n",
        "                   kernel_initializer='he_normal',\n",
        "                   use_bias=False,\n",
        "                   padding='same')(b2_out)\n",
        "    b3_u1 = BatchNormalization()(b3_u1)\n",
        "    b3_u1 = Activation(activation)(b3_u1)\n",
        "    b3_u1 = AveragePooling2D((2, 1))(b3_u1)\n",
        "    b3_u1 = Dropout(dropout_rate)(b3_u1)\n",
        "\n",
        "    b3_u2 = Conv2D(filters=int(filters_per_branch*len(scales_samples)/4),\n",
        "                   kernel_size=(4, 1),\n",
        "                   kernel_initializer='he_normal',\n",
        "                   use_bias=False,\n",
        "                   padding='same')(b3_u1)\n",
        "    b3_u2 = BatchNormalization()(b3_u2)\n",
        "    b3_u2 = Activation(activation)(b3_u2)\n",
        "    b3_u2 = AveragePooling2D((2, 1))(b3_u2)\n",
        "    b3_out = Dropout(dropout_rate)(b3_u2)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = Flatten()(b3_out)\n",
        "    output_layer = Dense(n_classes, activation='softmax')(output_layer)\n",
        "\n",
        "    # ================================ MODEL ================================= #\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9,\n",
        "                                      beta_2=0.999, amsgrad=False)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "W2caCCrQwpJH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Usage example of EEG-Inception with an ERP-based BCI dataset from:\n",
        "Download the dataset from:\n",
        "https://www.kaggle.com/esantamaria/gibuva-erpbci-dataset\n",
        "\"\"\"\n",
        "\n",
        "#%% IMPORT LIBRARIES\n",
        "import numpy as np\n",
        "import h5py, os\n",
        "#from EEGInception import EEGInception\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#%% PARAMETERS\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/gib uva erp/GIB-UVA ERP-BCI.hdf5'\n",
        "\n",
        "#%% HYPERPARAMETERS\n",
        "\n",
        "input_time = 1000\n",
        "fs = 128\n",
        "n_cha = 8\n",
        "filters_per_branch = 8\n",
        "scales_time = (500, 250, 125)\n",
        "dropout_rate = 0.25\n",
        "activation = 'elu'\n",
        "n_classes = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "#%% LOAD DATASET\n",
        "hf = h5py.File(dataset_path, 'r')\n",
        "features = np.array(hf.get(\"features\"))\n",
        "erp_labels = np.array(hf.get(\"erp_labels\"))\n",
        "codes = np.array(hf.get(\"codes\"))\n",
        "trials = np.array(hf.get(\"trials\"))\n",
        "sequences = np.array(hf.get(\"sequences\"))\n",
        "matrix_indexes = np.array(hf.get(\"matrix_indexes\"))\n",
        "run_indexes = np.array(hf.get(\"run_indexes\"))\n",
        "subjects = np.array(hf.get(\"subjects\"))\n",
        "database_ids = np.array(hf.get(\"database_ids\"))\n",
        "target = np.array(hf.get(\"target\"))\n",
        "matrix_dims = np.array(hf.get(\"matrix_dims\"))\n",
        "hf.close()\n",
        "\n",
        "#%% PREPARE FEATURES AND LABELS\n",
        "# Reshape epochs for EEG-Inception\n",
        "features = features.reshape(\n",
        "    (features.shape[0], features.shape[1],\n",
        "     features.shape[2], 1)\n",
        ")\n",
        "\n",
        "\n",
        "# One hot encoding of labels\n",
        "def one_hot_labels(categorical_labels):\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    on_hot_labels = enc.fit_transform(\n",
        "        categorical_labels.reshape(-1, 1)).toarray()\n",
        "    return on_hot_labels\n",
        "\n",
        "\n",
        "train_erp_labels = one_hot_labels(erp_labels)\n",
        "\n",
        "#%%  TRAINING\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "\n",
        "# Create model\n",
        "model = EEGInception(\n",
        "    input_time=1000, fs=128, ncha=8, filters_per_branch=8,\n",
        "    scales_time=(500, 250, 125), dropout_rate=0.25,\n",
        "    activation='elu', n_classes=2, learning_rate=0.001)\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0.0001,\n",
        "    mode='min', patience=10, verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "# Fit model\n",
        "fit_hist = model.fit(features,\n",
        "                     erp_labels,\n",
        "                     epochs=500,\n",
        "                     batch_size=1024,\n",
        "                     validation_split=0.2,\n",
        "                     callbacks=[early_stopping])\n",
        "\n",
        "# Save\n",
        "model.save('model')"
      ],
      "metadata": {
        "id": "0JfeEFAMxYat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94476ce5-ac8b-456a-af0f-3b9951e73037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 8, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 128, 8, 8)    520         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 128, 8, 8)    264         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 128, 8, 8)    136         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 8, 8)   32          ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 8, 8)   32          ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 128, 8, 8)   32          ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 8, 8)    0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 128, 8, 8)    0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 128, 8, 8)    0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128, 8, 8)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128, 8, 8)    0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128, 8, 8)    0           ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " depthwise_conv2d (DepthwiseCon  (None, 128, 1, 16)  128         ['dropout[0][0]']                \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " depthwise_conv2d_1 (DepthwiseC  (None, 128, 1, 16)  128         ['dropout_2[0][0]']              \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " depthwise_conv2d_2 (DepthwiseC  (None, 128, 1, 16)  128         ['dropout_4[0][0]']              \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 1, 16)  64          ['depthwise_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 1, 16)  64          ['depthwise_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 128, 1, 16)  64          ['depthwise_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128, 1, 16)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 128, 1, 16)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 128, 1, 16)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128, 1, 16)   0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 128, 1, 16)   0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128, 1, 16)   0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 1, 48)   0           ['dropout_1[0][0]',              \n",
            "                                                                  'dropout_3[0][0]',              \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 32, 1, 48)   0           ['concatenate[0][0]']            \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 1, 8)     6144        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 1, 8)     3072        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 1, 8)     1536        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 1, 8)    32          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 1, 8)    32          ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 1, 8)    32          ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 1, 8)     0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 1, 8)     0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 1, 8)     0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 32, 1, 8)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 32, 1, 8)     0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 32, 1, 8)     0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 1, 24)    0           ['dropout_6[0][0]',              \n",
            "                                                                  'dropout_7[0][0]',              \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 16, 1, 24)   0           ['concatenate_1[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 1, 12)    2304        ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 1, 12)   48          ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 1, 12)    0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 8, 1, 12)    0           ['activation_9[0][0]']           \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 8, 1, 12)     0           ['average_pooling2d_2[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 8, 1, 6)      288         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 8, 1, 6)     24          ['conv2d_7[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 8, 1, 6)      0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 4, 1, 6)     0           ['activation_10[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 4, 1, 6)      0           ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 24)           0           ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2)            50          ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,154\n",
            "Trainable params: 14,926\n",
            "Non-trainable params: 228\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "549/549 [==============================] - 85s 134ms/step - loss: 0.4429 - accuracy: 0.8213 - val_loss: 0.4737 - val_accuracy: 0.8048\n",
            "Epoch 2/500\n",
            "549/549 [==============================] - 72s 131ms/step - loss: 0.4140 - accuracy: 0.8321 - val_loss: 0.4695 - val_accuracy: 0.8060\n",
            "Epoch 3/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.4075 - accuracy: 0.8345 - val_loss: 0.4710 - val_accuracy: 0.8073\n",
            "Epoch 4/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.4036 - accuracy: 0.8356 - val_loss: 0.4681 - val_accuracy: 0.8080\n",
            "Epoch 5/500\n",
            "549/549 [==============================] - 72s 132ms/step - loss: 0.4012 - accuracy: 0.8368 - val_loss: 0.4705 - val_accuracy: 0.8067\n",
            "Epoch 6/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3995 - accuracy: 0.8373 - val_loss: 0.4705 - val_accuracy: 0.8060\n",
            "Epoch 7/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3977 - accuracy: 0.8378 - val_loss: 0.4730 - val_accuracy: 0.8063\n",
            "Epoch 8/500\n",
            "549/549 [==============================] - 73s 133ms/step - loss: 0.3964 - accuracy: 0.8382 - val_loss: 0.4697 - val_accuracy: 0.8065\n",
            "Epoch 9/500\n",
            "549/549 [==============================] - 72s 132ms/step - loss: 0.3954 - accuracy: 0.8384 - val_loss: 0.4682 - val_accuracy: 0.8076\n",
            "Epoch 10/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3945 - accuracy: 0.8387 - val_loss: 0.4709 - val_accuracy: 0.8069\n",
            "Epoch 11/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3938 - accuracy: 0.8392 - val_loss: 0.4688 - val_accuracy: 0.8068\n",
            "Epoch 12/500\n",
            "549/549 [==============================] - 73s 133ms/step - loss: 0.3933 - accuracy: 0.8392 - val_loss: 0.4696 - val_accuracy: 0.8054\n",
            "Epoch 13/500\n",
            "549/549 [==============================] - 74s 135ms/step - loss: 0.3929 - accuracy: 0.8395 - val_loss: 0.4689 - val_accuracy: 0.8064\n",
            "Epoch 14/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3918 - accuracy: 0.8399 - val_loss: 0.4665 - val_accuracy: 0.8055\n",
            "Epoch 15/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3919 - accuracy: 0.8399 - val_loss: 0.4691 - val_accuracy: 0.8064\n",
            "Epoch 16/500\n",
            "549/549 [==============================] - 72s 130ms/step - loss: 0.3911 - accuracy: 0.8401 - val_loss: 0.4690 - val_accuracy: 0.8066\n",
            "Epoch 17/500\n",
            "549/549 [==============================] - 73s 133ms/step - loss: 0.3908 - accuracy: 0.8403 - val_loss: 0.4681 - val_accuracy: 0.8067\n",
            "Epoch 18/500\n",
            "549/549 [==============================] - 72s 131ms/step - loss: 0.3905 - accuracy: 0.8405 - val_loss: 0.4698 - val_accuracy: 0.8053\n",
            "Epoch 19/500\n",
            "549/549 [==============================] - 72s 131ms/step - loss: 0.3903 - accuracy: 0.8404 - val_loss: 0.4680 - val_accuracy: 0.8078\n",
            "Epoch 20/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3898 - accuracy: 0.8408 - val_loss: 0.4744 - val_accuracy: 0.8052\n",
            "Epoch 21/500\n",
            "549/549 [==============================] - 72s 132ms/step - loss: 0.3895 - accuracy: 0.8409 - val_loss: 0.4677 - val_accuracy: 0.8077\n",
            "Epoch 22/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3887 - accuracy: 0.8415 - val_loss: 0.4668 - val_accuracy: 0.8069\n",
            "Epoch 23/500\n",
            "549/549 [==============================] - 71s 130ms/step - loss: 0.3886 - accuracy: 0.8411 - val_loss: 0.4672 - val_accuracy: 0.8070\n",
            "Epoch 24/500\n",
            "548/549 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8413Restoring model weights from the end of the best epoch: 14.\n",
            "549/549 [==============================] - 72s 131ms/step - loss: 0.3890 - accuracy: 0.8413 - val_loss: 0.4681 - val_accuracy: 0.8071\n",
            "Epoch 00024: early stopping\n",
            "INFO:tensorflow:Assets written to: model/assets\n"
          ]
        }
      ]
    }
  ]
}